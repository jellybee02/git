{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c7f6763",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ethan/anaconda3/envs/pl_llm/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np, pandas as pd, warnings, os, openai, json\n",
    "from tqdm.auto import tqdm\n",
    "from openai import OpenAI\n",
    "warnings.filterwarnings('ignore')\n",
    "import requests\n",
    "from typing import List, Tuple, Union\n",
    "from langchain_ollama.embeddings import OllamaEmbeddings\n",
    "import pickle\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "# from haversine import haversine, Unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ec557a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/prep3.csv\", encoding='cp949')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33b3737d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ethan/git_rp/git/LLM/script'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b2d335",
   "metadata": {},
   "source": [
    "# API Key & Base URL:A.company 5070TI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aaa68fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = open('../../../../api_key.txt','r')\n",
    "api_key = key.read()\n",
    "openai.api_key = api_key\n",
    "\n",
    "base_ = open('../../../../base_url.txt','r')\n",
    "base_url = base_.read()\n",
    "# openai.api_key = api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9cc649",
   "metadata": {},
   "source": [
    "# Embedding Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d155355f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "245721bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OllamaSentenceTransformer():\n",
    "    def __init__(\n",
    "            self,\n",
    "            *args,\n",
    "            **kargs,\n",
    "            ) -> None:\n",
    "                self.base_url = kargs.get(\"base_url\", \"http://localhost:11434\")\n",
    "                self.model = kargs.get(\"model\",\"bge-m3\")\n",
    "                self.embedding_model = OllamaEmbeddings(\n",
    "                            model = self.model,\n",
    "                            base_url = self.base_url,\n",
    "                        )\n",
    "                        \n",
    "    \n",
    "    def encode(\n",
    "            self,\n",
    "            documents:Union[str, List[str], np.ndarray],\n",
    "            *args,\n",
    "            **kargs,\n",
    "        )-> np.ndarray:\n",
    "        if isinstance(documents, str):\n",
    "            document_embeddings = self.embedding_model.embed_query(documents)\n",
    "            return np.array(document_embeddings)\n",
    "        \n",
    "        if isinstance(documents, np.ndarray):\n",
    "            documents = documents.tolist()\n",
    "        \n",
    "        document_embeddings = [self.embedding_model.embed_query(s) for s in documents]\n",
    "        return np.array(document_embeddings)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e46af4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_transformer = OllamaSentenceTransformer(base_url=base_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "523ef9a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.02764925,  0.00978895, -0.03736658, ..., -0.01262593,\n",
       "         0.01433419,  0.0590205 ],\n",
       "       [-0.0051665 ,  0.04774755, -0.02180332, ..., -0.0344875 ,\n",
       "        -0.00380494,  0.05704403],\n",
       "       [-0.03296679,  0.05047558, -0.05491041, ...,  0.01124008,\n",
       "         0.04521679,  0.10267203]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_transformer.encode(['123','24','234'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb27d41a",
   "metadata": {},
   "source": [
    "# Input Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d911b34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tool_name': '전동드릴',\n",
       " 'location': '강남역',\n",
       " 'job_content': '전동드릴을 사용해서 나무를 고정하고 싶어'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../data/user_input_content.json', \"rb\") as f:\n",
    "    input_content = json.load(f)\n",
    "\n",
    "input_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae7affd",
   "metadata": {},
   "source": [
    "# Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4c1d54a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = f\"\"\"나는 {input_content['tool_name']}을(를) 빌리고 싶어. \n",
    "그리고 이 도구를 이용해서 하려는 작업은 '{input_content['job_content']}' 이야. \n",
    "해당 작업을 하면서 같이 빌리면 좋은 공구도 함께 알려줘\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e86b83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7aa62a71",
   "metadata": {},
   "source": [
    "# LLM Define\n",
    "## Ver.OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ac4beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def invoke(question, api_key=api_key, model=\"gpt-4o\", temperature=0.7):\n",
    "    client = OpenAI(api_key=api_key)\n",
    "    \n",
    "    system_content = (\n",
    "        \"\"\"너는 사람들의 질문에 친절히 답해주는 도우미야.\n",
    "            사용자의 질문에 친절히 답해줘.\"\"\"\n",
    "    )\n",
    "\n",
    "    # 메시지 구성\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_content},\n",
    "        {\"role\": \"user\", \"content\": question}\n",
    "    ]\n",
    "    # 스트리밍 요청\n",
    "    stream = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "        stream=True  # ⭐ 핵심 옵션!\n",
    "    )\n",
    "\n",
    "    # generator 반환 (chunk 단위 텍스트 출력)\n",
    "    return stream\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e21c0be",
   "metadata": {},
   "source": [
    "### Output the response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cdb74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in invoke(question):\n",
    "    if chunk.choices[0].delta.content:\n",
    "        print(chunk.choices[0].delta.content, end='', flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2229fc73",
   "metadata": {},
   "source": [
    "## Ver.Gemma3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f84ca30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama.chat_models import ChatOllama\n",
    "\n",
    "def invoke(question, info = None, model=\"gemma3:12b\", temperature=0.7):\n",
    "    chat = ChatOllama(model=model,\n",
    "                      base_url=base_url,\n",
    "                      temperature=temperature,)\n",
    "\n",
    "    system_content = (\n",
    "                \"\"\"너는 사람들의 질문에 친절히 답해주는 도우미야.\n",
    "                    사용자의 질문에 친절히 답해줘.\"\"\"\n",
    "        )\n",
    "\n",
    "    # 메시지 구성\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_content},\n",
    "        {\"role\": \"user\", \"content\": question}\n",
    "    ]\n",
    "\n",
    "    # invoke로 전체 응답 받기\n",
    "    # response = chat.invoke(messages).content\n",
    "    response = chat.stream(messages)\n",
    "\n",
    "    # 응답 내용 반환\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28880df",
   "metadata": {},
   "source": [
    "### Output the response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c051fa12",
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in invoke(question):\n",
    "    print(chunk.content, end='', flush=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pl_llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
