{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c7f6763",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ethan/anaconda3/envs/pl_llm/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np, pandas as pd, warnings, os, openai, json\n",
    "from tqdm.auto import tqdm\n",
    "from openai import OpenAI\n",
    "warnings.filterwarnings('ignore')\n",
    "import requests\n",
    "from typing import List, Tuple, Union\n",
    "from langchain_ollama.embeddings import OllamaEmbeddings\n",
    "import pickle\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "# from haversine import haversine, Unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ec557a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/prep3.csv\", encoding='cp949')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33b3737d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ethan/git_rp/git/LLM/script'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b2d335",
   "metadata": {},
   "source": [
    "# API Key & Base URL:A.company 5070TI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aaa68fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = open('../../../../api_key.txt','r')\n",
    "api_key = key.read()\n",
    "openai.api_key = api_key\n",
    "\n",
    "base_ = open('../../../../base_url.txt','r')\n",
    "base_url = base_.read()\n",
    "# openai.api_key = api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9cc649",
   "metadata": {},
   "source": [
    "# Embedding Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d155355f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "245721bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OllamaSentenceTransformer():\n",
    "    def __init__(\n",
    "            self,\n",
    "            *args,\n",
    "            **kargs,\n",
    "            ) -> None:\n",
    "                self.base_url = kargs.get(\"base_url\", \"http://localhost:11434\")\n",
    "                self.model = kargs.get(\"model\",\"bge-m3\")\n",
    "                self.embedding_model = OllamaEmbeddings(\n",
    "                            model = self.model,\n",
    "                            base_url = self.base_url,\n",
    "                        )\n",
    "                        \n",
    "    \n",
    "    def encode(\n",
    "            self,\n",
    "            documents:Union[str, List[str], np.ndarray],\n",
    "            *args,\n",
    "            **kargs,\n",
    "        )-> np.ndarray:\n",
    "        if isinstance(documents, str):\n",
    "            document_embeddings = self.embedding_model.embed_query(documents)\n",
    "            return np.array(document_embeddings)\n",
    "        \n",
    "        if isinstance(documents, np.ndarray):\n",
    "            documents = documents.tolist()\n",
    "        \n",
    "        document_embeddings = [self.embedding_model.embed_query(s) for s in documents]\n",
    "        return np.array(document_embeddings)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e46af4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_transformer = OllamaSentenceTransformer(base_url=base_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "523ef9a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.02764925,  0.00978895, -0.03736658, ..., -0.01262593,\n",
       "         0.01433419,  0.0590205 ],\n",
       "       [-0.0051665 ,  0.04774755, -0.02180332, ..., -0.0344875 ,\n",
       "        -0.00380494,  0.05704403],\n",
       "       [-0.03296679,  0.05047558, -0.05491041, ...,  0.01124008,\n",
       "         0.04521679,  0.10267203]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_transformer.encode(['123','24','234'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb27d41a",
   "metadata": {},
   "source": [
    "# Input Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1d911b34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tool_name': 'ì „ë™ë“œë¦´',\n",
       " 'location': 'ê°•ë‚¨ì—­',\n",
       " 'job_content': 'ì „ë™ë“œë¦´ì„ ì‚¬ìš©í•´ì„œ ë‚˜ë¬´ë¥¼ ê³ ì •í•˜ê³  ì‹¶ì–´'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../data/user_input_content.json', \"rb\") as f:\n",
    "    input_content = json.load(f)\n",
    "\n",
    "input_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae7affd",
   "metadata": {},
   "source": [
    "# Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4c1d54a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = f\"\"\"ë‚˜ëŠ” {input_content['tool_name']}ì„(ë¥¼) ë¹Œë¦¬ê³  ì‹¶ì–´. \n",
    "ê·¸ë¦¬ê³  ì´ ë„êµ¬ë¥¼ ì´ìš©í•´ì„œ í•˜ë ¤ëŠ” ì‘ì—…ì€ '{input_content['job_content']}' ì´ì•¼. \n",
    "í•´ë‹¹ ì‘ì—…ì„ í•˜ë©´ì„œ ê°™ì´ ë¹Œë¦¬ë©´ ì¢‹ì€ ê³µêµ¬ë„ í•¨ê»˜ ì•Œë ¤ì¤˜\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e86b83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7aa62a71",
   "metadata": {},
   "source": [
    "# LLM Define\n",
    "## Ver.OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ac4beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def invoke(question:str,\n",
    "           history:str = None, \n",
    "           api_key:str=api_key, \n",
    "           model:str=\"gpt-4o\", \n",
    "           temperature:float=0.7):\n",
    "    client = OpenAI(api_key=api_key)\n",
    "    \n",
    "    system_content = (\n",
    "        \"\"\"ë„ˆëŠ” ì‚¬ëŒë“¤ì˜ ì§ˆë¬¸ì— ì¹œì ˆíˆ ë‹µí•´ì£¼ëŠ” ë„ìš°ë¯¸ì•¼.\n",
    "            ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì¹œì ˆíˆ ë‹µí•´ì¤˜.\"\"\"\n",
    "    )\n",
    "    messages = [{\"role\": \"system\", \"content\": system_content}]\n",
    "    \n",
    "    # ì´ì „ ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ í¬í•¨\n",
    "    if history:\n",
    "        messages.extend(history)\n",
    "\n",
    "    # ë§ˆì§€ë§‰ ì§ˆë¬¸ ì¶”ê°€\n",
    "    messages.append({\"role\": \"user\", \"content\": question})\n",
    "    \n",
    "    \n",
    "    # ìŠ¤íŠ¸ë¦¬ë° ìš”ì²­\n",
    "    stream = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "        stream=True  # â­ í•µì‹¬ ì˜µì…˜!\n",
    "    )\n",
    "\n",
    "    # generator ë°˜í™˜ (chunk ë‹¨ìœ„ í…ìŠ¤íŠ¸ ì¶œë ¥)\n",
    "    return stream\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cd77cdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_content = (\n",
    "        \"\"\"ë„ˆëŠ” ì‚¬ëŒë“¤ì˜ ì§ˆë¬¸ì— ì¹œì ˆíˆ ë‹µí•´ì£¼ëŠ” ë„ìš°ë¯¸ì•¼.\n",
    "            ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì¹œì ˆíˆ ë‹µí•´ì¤˜.\"\"\"\n",
    "    )\n",
    "messages = [{\"role\": \"system\", \"content\": system_content}]\n",
    "\n",
    "messages.extend([{\"role\": \"user\", \"content\": 'user_input'},\n",
    "                 {\"role\": \"system\", \"content\": 'response_text'}\n",
    "                 ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0523ca9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': 'ë„ˆëŠ” ì‚¬ëŒë“¤ì˜ ì§ˆë¬¸ì— ì¹œì ˆíˆ ë‹µí•´ì£¼ëŠ” ë„ìš°ë¯¸ì•¼.\\n            ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì¹œì ˆíˆ ë‹µí•´ì¤˜.'},\n",
       " {'role': 'user', 'content': 'user_input'},\n",
       " {'role': 'system', 'content': 'response_text'}]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad7adbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2e21c0be",
   "metadata": {},
   "source": [
    "### Output the response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cdb74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in invoke(question):\n",
    "    if chunk.choices[0].delta.content:\n",
    "        print(chunk.choices[0].delta.content, end='', flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3830cbac",
   "metadata": {},
   "source": [
    "### Response based History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1b08ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = []\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"\\nğŸ™‚ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”: \")\n",
    "    if user_input.lower() in [\"exit\", \"quit\", \"ì¢…ë£Œ\", \"ë\"]:\n",
    "        break\n",
    "\n",
    "    print(\"ğŸ§  GPT ì‘ë‹µ:\")\n",
    "    response_text = \"\"\n",
    "    \n",
    "    for chunk in invoke(user_input, history=history, api_key=api_key):\n",
    "        delta = chunk.choices[0].delta\n",
    "        if delta.content:\n",
    "            print(delta.content, end=\"\", flush=True)\n",
    "            response_text += delta.content\n",
    "\n",
    "    # ì‘ë‹µ í›„ ëŒ€í™” ì´ë ¥ ì—…ë°ì´íŠ¸\n",
    "    history.append({\"role\": \"user\", \"content\": user_input})\n",
    "    history.append({\"role\": \"system\", \"content\": response_text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b989beca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2229fc73",
   "metadata": {},
   "source": [
    "## Ver.Gemma3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f84ca30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama.chat_models import ChatOllama\n",
    "\n",
    "def invoke(question, info = None, model=\"gemma3:12b\", temperature=0.7):\n",
    "    chat = ChatOllama(model=model,\n",
    "                      base_url=base_url,\n",
    "                      temperature=temperature,)\n",
    "\n",
    "    system_content = (\n",
    "                \"\"\"ë„ˆëŠ” ì‚¬ëŒë“¤ì˜ ì§ˆë¬¸ì— ì¹œì ˆíˆ ë‹µí•´ì£¼ëŠ” ë„ìš°ë¯¸ì•¼.\n",
    "                    ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì¹œì ˆíˆ ë‹µí•´ì¤˜.\"\"\"\n",
    "        )\n",
    "\n",
    "    messages = [{\"role\": \"system\", \"content\": system_content}]\n",
    "    \n",
    "    # ì´ì „ ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ í¬í•¨\n",
    "    if history:\n",
    "        messages.extend(history)\n",
    "\n",
    "    # ë§ˆì§€ë§‰ ì§ˆë¬¸ ì¶”ê°€\n",
    "    messages.append({\"role\": \"user\", \"content\": question})\n",
    "    \n",
    "\n",
    "    # invokeë¡œ ì „ì²´ ì‘ë‹µ ë°›ê¸°\n",
    "    # response = chat.invoke(messages).content\n",
    "    response = chat.stream(messages)\n",
    "\n",
    "    # ì‘ë‹µ ë‚´ìš© ë°˜í™˜\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28880df",
   "metadata": {},
   "source": [
    "### Output the response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c051fa12",
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in invoke(question):\n",
    "    print(chunk.content, end='', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1b87a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = []\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"\\nğŸ™‚ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”: \")\n",
    "    if user_input.lower() in [\"exit\", \"quit\", \"ì¢…ë£Œ\", \"ë\"]:\n",
    "        break\n",
    "\n",
    "    print(\"ğŸ§  GPT ì‘ë‹µ:\")\n",
    "    response_text = \"\"\n",
    "    \n",
    "    for chunk in invoke(user_input, history=history, api_key=api_key):\n",
    "        delta = chunk.choices[0].delta\n",
    "        if delta.content:\n",
    "            print(delta.content, end=\"\", flush=True)\n",
    "            response_text += delta.content\n",
    "\n",
    "    # ì‘ë‹µ í›„ ëŒ€í™” ì´ë ¥ ì—…ë°ì´íŠ¸\n",
    "    history.append({\"role\": \"user\", \"content\": user_input})\n",
    "    history.append({\"role\": \"system\", \"content\": response_text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5410f2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pl_llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
